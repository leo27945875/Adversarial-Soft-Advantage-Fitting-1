```math
# Adversarial Soft Advantage Fitting å­¸ç¿’ç­†è¨˜

# ç°¡ä»‹ASAF

   **[Adversarial Soft Advantage Fitting (ASAF)](https://arxiv.org/abs/2006.13258)** æ˜¯ä¸€ç¨®**Imitation Learning (IL)**æŠ€è¡“ï¼Œå¯ä»¥é€éè§€å¯Ÿexpert (å¯ä»¥æ˜¯äººé¡ã€AIæ¨¡å‹... ç­‰)ï¼Œé€²è€Œå­¸ç¿’æ¨¡ä»¿expertçš„è¡Œç‚ºã€‚

  **èˆ‡å…¶ä»–ILæŠ€è¡“ç›¸æ¯”ï¼ŒASAFå…·æœ‰ä»¥ä¸‹å¹¾é»ç‰¹è‰²:** 

- å…·æœ‰discriminatorèˆ‡generator (å³policy)çš„æ¶æ§‹ï¼Œå±¬æ–¼**Adversarial Imitation Learning (AIL)**çš„ä¸€ç¨®ã€‚
- è¨“ç·´å¥½discriminatorçš„åŒæ™‚ï¼Œå¯åŒæ™‚å¾—åˆ°è¨“ç·´å¥½çš„generatorï¼Œ**çœå»policy optimization**çš„æ­¥é©Ÿã€‚
- ç”±æ–¼æ²’æœ‰policy optimizationçš„æ­¥é©Ÿï¼Œå› æ­¤è¨“ç·´éç¨‹æ›´åŠ **ç©©å®š**ä¸”**å¿«é€Ÿ**ã€‚

# ä½•è¬‚**Adversarial Imitation Learning (AIL) ?**

![A.png](image/A.png)

**ä¸Šåœ–ç‚ºAILçš„åŸºæœ¬æ¶æ§‹ï¼Œå…¶ä¸­:**

$$\pi_e: \text{expert policy}, \ \pi: \text{learned policy}, \\
\tau_e: \text{expert trajectory}, \ \tau: \text{generated trajectory}$$

å¾æµç¨‹ä¸­ä¸é›£çœ‹å‡ºï¼Œå…¶å¯¦R(s, a)çš„è§’è‰²å°±æ˜¯discriminatorå€åˆ†$\tau_e$èˆ‡$\tau$ï¼Œè€Œpolicy $\pi$ å°±æ˜¯generatorè©¦åœ–ç”¢ç”Ÿèˆ‡expertå¾ˆåƒçš„trajectoryå»é¨™éR(s, a)ã€‚

ä¹Ÿå°±æ˜¯å› ç‚ºåœ¨AILè¨“ç·´éç¨‹ä¸­ï¼Œreward function R(s, a)æœƒé€éè§€å¯Ÿexpertçš„trajectoriesè¢«è¨“ç·´å‡ºä¾†ï¼Œæˆ‘å€‘å°±å†ä¹Ÿä¸å¿…åƒä¸€èˆ¬çš„RL taskéœ€è¦è‡ªå·±è¨­è¨ˆreward functionã€‚

**å…¶å¯¦æ•´å€‹éç¨‹åœ¨åšçš„äº‹æƒ…ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯:**

1. å…ˆlearnå‡ºä¸€å€‹reward function **R(s, a)å¯ä»¥çµ¦$\tau_e$ rewardè¶Šå¤§è¶Šå¥½ï¼Œè€Œçµ¦$\tau$ rewardè¶Šä½è¶Šå¥½**ã€‚
2. åœ¨æ–°è¨“ç·´å‡ºçš„R(s, a)çš„åŸºç¤ä¸Šï¼Œä»¥**ä¸€èˆ¬RLæ¼”ç®—æ³•**è¨“ç·´æ›´æ–°policy $\pi$ã€‚
3. é‡è¤‡æ­¥é©Ÿa. b.ç›´è‡³çµæŸã€‚  
<br/>

# å…è²»çš„Policy Optimization ?

åœ¨ç°¡ä»‹ä¸­æœ‰èªªï¼ŒASAFèˆ‡ä¸€èˆ¬çš„AILä¸ä¸€æ¨£ï¼Œå®ƒç„¡é ˆè¨“ç·´policyçš„éç¨‹å°±å¯å¾—åˆ°è¨“ç·´å¥½çš„policyï¼Œé€™åˆ°åº•æ˜¯ç”šéº¼ç¥å¥‡çš„å·«è¡“ğŸ˜¨ ?  ä»¥ä¸‹å°±é–‹å§‹è¨è«–å…¶æ•¸å­¸åŸç†ã€‚

## (ä¸€) é¦–å…ˆå¾GANçš„optimal discriminatorèªªèµ·:
æˆ‘å€‘çŸ¥é“GANçš„objectiveçš„å¼å­å¦‚ä¸‹:
$$L\left(D, G\right) \triangleq \mathbb{E}_{x \sim p_{data}}[\log D(x)]+\mathbb{E}_{x \sim p_{G}}[\log (1-D(x))] $$
ä¸¦ä¸”ä»¥æ­¤ç‚ºåŸºç¤ï¼Œç¶“éä¸€äº›æ•¸å­¸è­‰æ˜å¾Œæˆ‘å€‘å¯å¾—çŸ¥:
$$D_{p_{G}}^{*} \triangleq \underset{D}{\arg \max } L\left(D, G\right)=\frac{p_{data}}{p_{data}+p_{G}}$$
å¯ä»¥å…ˆè¨˜ä½é€™å€‹optimal discriminatorçš„å½¢å¼ï¼Œé€™æœƒæ˜¯è¡ç”Ÿå‡ºå¾Œé¢æ‰€æœ‰æ±è¥¿çš„é‡è¦åŸºç¤ !  
<br/>

## (äºŒ) æ€éº¼ä¸ä¹¾è„†ç›´æ¥ç”¨$D^*_{p_G}$å°±å¥½ğŸ˜€~

æˆ‘å€‘çŸ¥é“GANçš„objectiveçš„å¼å­å¦‚ä¸‹:     

ASAFä½œè€…å—åˆ°ä¸€äº›[**å…¶ä»–AILè«–æ–‡**](https://arxiv.org/abs/1611.03852)çš„å•Ÿç™¼ï¼Œæ±ºå®šç›´æ¥æŠŠoptimal discriminatorçš„å½¢å¼æ‹¿ä¾†modelè‡ªå·±å‡è¨­çš„discriminatorï¼Œä¸¦ç¨±ä¹‹ç‚º **structured disciminator** :  ****

$$                L\left(D, G\right) \triangleq \mathbb{E}_{x \sim p_{data}}[\log D(x)]+\mathbb{E}_{x \sim p_{G}}[\log (1-D(x))] $$

$$D_{\tilde{p}, p_{G}}(x)=\frac{\tilde{p}(x)}{\tilde{p}(x)+p_{G}(x)}  $$

ä¸¦ä¸”ä»¥æ­¤ç‚ºåŸºç¤ï¼Œç¶“éä¸€äº›æ•¸å­¸è­‰æ˜å¾Œæˆ‘å€‘å¯å¾—çŸ¥:

ä¸¦ä¸”å†å°‡GANçš„objectiveæ”¹å¯«ç‚º: 

$$                D_{p_{G}}^{*} \triangleq \underset{D}{\arg \max } L\left(D, G\right)=\frac{p_{data}}{p_{data}+p_{G}}$$

å¯ä»¥å…ˆè¨˜ä½é€™å€‹optimal discriminatorçš„å½¢å¼ï¼Œé€™æœƒæ˜¯è¡ç”Ÿå‡ºå¾Œé¢æ‰€æœ‰æ±è¥¿çš„é‡è¦åŸºç¤ !

$$L\left(\tilde{p}, p_{G}\right) \triangleq \mathbb{E}_{x \sim p_{E}}\left[\log D_{\tilde{p}, p_{G}}(x)\right]+\mathbb{E}_{x \sim p_{G}}\left[\log \left(1-D_{\tilde{p}, p_{G}}(x)\right)\right]$$

å…¶ä¸­ï¼Œ $p_E$ç‚ºçœŸå¯¦data ( å³expertè¡Œç‚ºçš„distribution ) ï¼Œ$p_G$ç”Ÿæˆdata ( å³policyè¡Œç‚ºçš„distribution )ï¼Œ$\tilde{p}$æ˜¯learnableçš„distributionï¼Œè€Œdiscriminatoræ˜¯parameterized by  $\tilde{p}$ã€‚æˆ‘å€‘å¾ˆå¿«å°±æœƒçœ‹åˆ°é€™éº¼å‡è¨­çš„å¥½è™•ã€‚  
<br>

## (ä¸‰) ä¸€äº›å¥½åƒä¸æ€éº¼æ„å¤–çš„æ•¸å­¸å»è®“å¥‡è¹Ÿç™¼ç”Ÿäº† !

![B.png](image/B.png)

åˆ°é€™é‚Š ASAFçš„**æœ€æ ¸å¿ƒçš„å¼•ç†**çµ‚æ–¼å‡ºç¾äº† !  ä¸Šé¢é€™å€‹**Lemma 1.**ç”¨ç™½è©±æ–‡èªªå°±æ˜¯ï¼Œç•¶å»æœ€ä½³åŒ–æˆ‘å€‘å‰›å¾—åˆ°çš„æ–°objective $L(\tilde{p}, p_G)$ æ™‚ï¼Œå¯ä»¥è­‰æ˜optimal $\tilde{p}$ ( å³$**\tilde{p}^***$ )æœƒèˆ‡çœŸå¯¦dataçš„distribution ( å³$p_E$ )æ˜¯ç›¸ç­‰çš„ï¼Œä¸¦ä¸”æ›´é€²ä¸€æ­¥ï¼Œé‚„å¯ä»¥è­‰æ˜optimal $p_G$ ( å³$**p_G^***$ )ä¹Ÿæœƒç­‰æ–¼$p_E$ ã€‚

é›–ç„¶çœ‹ä¼¼æ˜¯æ²’ä»€éº¼é©šå–œçš„çµæœï¼Œä½†é€™æ®µæ•¸å­¸å‘Šè¨´æˆ‘å€‘ä¸€ä»¶å¾ˆé‡è¦çš„äº‹æƒ…ï¼Œä¹Ÿå°±æ˜¯ç•¶æ‰¾åˆ°$\tilde{p}^*$çš„åŒæ™‚ï¼Œå…¶å¯¦æˆ‘å€‘ä¹Ÿ**é †ä¾¿**æ‰¾åˆ°äº†$**p_G^*$ï¼Œ**å› ç‚ºç•¶ $\min_{p_G} \max_{\tilde{p}} L(\tilde{p}, p_G)$è¢«è§£å‡ºæ™‚ï¼Œ$**p_G^* = \tilde{p} = p_E**$ ã€‚é€™ä¹Ÿæ˜¯æä¾›äº†æˆ‘å€‘å¾Œé¢è­‰æ˜ASAFå¯ä»¥çœå»policy optimizationé€™å€‹é‡è¦æ­¥é©Ÿçš„ç†è«–åŸºç¤ !

ç¾åœ¨çœ‹å‘ä¸‹é¢çš„å¼å­ï¼Œç‚ºäº†è®“policyé€™å€‹é‡è¦è§’è‰²å‡ºä¾†éœ²è‡‰ï¼Œæˆ‘å€‘è¦å°‡$D_{\tilde{p}, p_{G}}(x)$æ”¹å¯«ç‚º$D_{\tilde{\pi}, \pi_{G}}(\tau)$:

$$D_{\tilde{\pi}, \pi_{G}}(\tau)=\frac{P_{\tilde{\pi}}(\tau)}{P_{\tilde{\pi}}(\tau)+P_{\pi_{G}}(\tau)}$$

æˆ‘å€‘çŸ¥é“$P_{\pi}(\tau) = P_0(s_0) \prod_{t=0}^{T-1} \pi(a_t|s_t) P(s_{t+1}|s_t, a_t)$ï¼Œè€Œ$D_{\tilde{\pi}, \pi_{G}}(\tau)$åˆ†å­åˆ†æ¯å¯å°‡$P_0(s_0) \prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t)$ç´„åˆ†æ‰ï¼Œå¾è€Œä½¿å¾—$D_{\tilde{\pi}, \pi_{G}}(\tau)$æœ€å¾Œçš„å½¢å¼è®Šæˆäº†æ¯å€‹é …éƒ½æ˜¯ä¸€æ¢trajectoryä¸­çš„policyæ©Ÿç‡å€¼é€£ä¹˜ï¼Œç›¸ç•¶ç°¡å–®æ˜“æ‡‚:

$$D_{\tilde{\pi}, \pi_{G}}(\tau)=\frac{q_{\tilde{\pi}}(\tau)}{q_{\tilde{\pi}}(\tau)+q_{\pi_{G}}(\tau)} \ , \ \text{å…¶ä¸­ } q_{\pi}(\tau) \triangleq \prod_{t=0}^{T-1} \pi\left(a_{t} \mid s_{t}\right)$$

æœ€å¾Œï¼Œé€éLemma 1.æˆ‘å€‘å¾—åˆ°æœ€é‡è¦çš„çµè«–:

$$\qquad \qquad \quad \ \ (a) \quad \tilde{\pi}^{*} \triangleq \underset{\tilde{\pi}}{\arg \max } L\left(\tilde{\pi}, \pi_{G}\right)  \ \text{satisfies} \ \ q_{\tilde{\pi}^{*}} = q_{\pi_{E}}\\ 
(b) \quad \pi_{G}^{*}=\tilde{\pi}^{*} \in \underset{\pi_{G}}{\arg \min } L\left(\tilde{\pi}^{*}, \pi_{G}\right)$$

# ASAFæ¼”ç®—æ³•

**ä»¥ä¸‹å°±æ˜¯æ•´å€‹æ¼”ç®—æ³•æµç¨‹ï¼Œå¯ä»¥çœ‹å‡ºå°‘äº†policy optimizationè®“æ•´å€‹æµç¨‹è®Šå¾—ç›¸ç•¶ç°¡æ½”ã€‚**

![AA.png](image/AA.png)

è€Œæ¼”ç®—æ³•ç¬¬5è¡Œæåˆ°çš„**Eq.(12)**å°±æ˜¯:

$$\begin{aligned}&\mathcal{L}_{B C E}\left(\mathcal{D}_{E}, \mathcal{D}_{G}, \tilde{\pi}\right) \approx-\frac{1}{n_{E}} \sum_{i=1}^{n_{E}} \log D_{\tilde{\pi}, \pi_{G}}\left(\tau_{i}^{(E)}\right)-\frac{1}{n_{G}} \sum_{i=1}^{n_{G}} \log \left(1-D_{\tilde{\pi}, \pi_{G}}\left(\tau_{i}^{(G)}\right)\right) \\&\text { where } \quad \tau_{i}^{(E)} \sim \mathcal{D}_{E}, \tau_{i}^{(G)} \sim \mathcal{D}_{G} \text { and } D_{\tilde{\pi}, \pi_{G}}(\tau)=\frac{\prod_{t=0}^{T-1} \tilde{\pi}\left(a_{t} \mid s_{t}\right)}{\prod_{t=0}^{T-1} \tilde{\pi}\left(a_{t} \mid s_{t}\right)+\prod_{t=0}^{T-1} \pi_{G}\left(a_{t} \mid s_{t}\right)}\end{aligned}$$

é€™å€‹æ¼”ç®—æ³•æœ‰ä¸€å€‹åœ°æ–¹æ²’èªªæ¸…æ¥šï¼Œç¬¬5è¡Œçš„updateå…¶å¯¦æ˜¯æœƒupdateå¥½å¹¾æ¬¡ï¼Œä¹Ÿå°±æ˜¯ç¬¬5è¡Œå…¶å¯¦æ˜¯åŒ…å«äº†é›™é‡è¿´åœˆï¼Œç¬¬ä¸€å€‹è¿´åœˆæ˜¯å±¬æ–¼epochsï¼Œç¬¬äºŒå€‹è¿´åœˆæ˜¯å±¬æ–¼batchsï¼Œä¸¦ä¸”expertè·Ÿagentçš„$\prod_{t=0}^{T-1} \pi_{G}\left(a_{t} \mid s_{t}\right)$åœ¨é€²å…¥é›™é‡è¿´åœˆå‰è¦äº‹å…ˆæ±‚å‡ºã€‚

ASAFå¦å¤–é‚„æœ‰2ç¨®è®Šå½¢: **ASAF-w**èˆ‡**ASAF-1**ï¼ŒASAF-wæ˜¯æŒ‡ä¸æœƒä¸€æ¬¡æŠŠæ•´æ¢trajectoryè¼¸å…¥$D_{\tilde{\pi}, \pi_G}$ï¼Œè€Œæ˜¯ä»¥æ¯wå€‹é€£çºŒçš„transitionsç‚ºä¸€çµ„è¼¸å…¥$D_{\tilde{\pi}, \pi_G}$ä¸­ï¼Œè€ŒASAF-1æ˜¯ç‰¹æŒ‡w = 1çš„æƒ…æ³ã€‚

æ¨æ¸¬é€™éº¼è®Šå½¢çš„åŸå› å¯èƒ½æ˜¯ç‚ºäº†**è§£æ±ºtrajectoryç„¡é™é•·**æˆ–æ˜¯è€ƒæ…®åˆ°**åˆ†å­åˆ†æ¯éƒ½æ˜¯ä¸€ä¸²å°æ–¼0çš„æ•¸é€£ä¹˜å¯èƒ½å°è‡´çš„ä¸ç©©å®š**ã€‚è€Œè«–æ–‡çš„å¯¦é©—çµæœä¹Ÿé¡¯ç¤ºASAF-wçš„performanceæ™®éè¼ƒåŸç‰ˆASAFé«˜ï¼Œå°¤å…¶ASAF-1æ˜¯æœ€å¥½çš„ã€‚

# ASAFè«–æ–‡å¯¦é©—çµæœ

## è«–æ–‡ä¸­**é€£çºŒaction spaceçš„å¯¦é©—çµæœåœ–:**

![AA.png](image/AA%201.png)

## ****è«–æ–‡ä¸­**å¯ä»¥è­‰æ˜ASAFæ˜¯åœ¨æ¨¡ä»¿expertçš„åœ–:**

![CC.png](image/CC.png)

### å¯çœ‹å‡ºæ¨¡ä»¿çš„expertæ„ˆå„ªç§€ï¼Œè¨“ç·´å‡ºä¾†çš„policyä¹Ÿæ„ˆå¥½ã€‚

# æˆ‘å€‘è‡ªå·±å¯¦ç¾çš„ASAF-1

æ³¨æ„ä»¥ä¸‹çš„åœ–çš†ç‚ºEWMAåœ– ( $\lambda$ = 0.02 )ã€‚

## **å¯¦é©—ä¸€ :   æ¨¡å‹å£“ç¸®æ‡‰ç”¨**

![SS.png](image/SS.png)

æˆ‘å€‘èªç‚ºimitation learningå¯ä»¥ç”¨ä¾†åšæ¨¡å‹å£“ç¸®ï¼Œè—‰ç”±ç”¨åƒæ•¸é‡å°‘çš„modelå»æ¨¡ä»¿åƒæ•¸é‡å¤šçš„modelï¼Œä»¥æœŸè®“å°modelå¯ä»¥æ“æœ‰èˆ‡å¤§modelç›¸åŒçš„performanceã€‚

æœ¬å¯¦é©—ä½¿ç”¨çš„ç’°å¢ƒç‚ºMuJoCo Ant-v2ï¼Œä¸¦ä¸”è¢«æ¨¡ä»¿çš„modelç‚º[TD3](https://arxiv.org/pdf/1802.09477.pdf)ï¼Œæ¨¡å‹ä¸­é–“æœ‰2å±¤unitæ•¸åˆ†åˆ¥ç‚º(400, 300)çš„hidden layersã€‚æˆ‘å€‘åˆ†åˆ¥ä½¿ç”¨(256, 256)ã€(200, 200)ã€(150, 150)ã€(128, 128)ã€(64, 64)çš„è¼ƒå°modelä¾†åšimitation learningã€‚

æœ€å¾Œçµæœå¯ä»¥çœ‹åˆ°ï¼Œ(256, 256)ã€(200, 200)ã€(150, 150)éƒ½å¯ä»¥æ¨¡ä»¿å¾—ç›¸ç•¶å¥½ï¼Œå…¶ä»–æ›´å°çš„modelå‰‡ç„¡æ³•ï¼Œè—‰ç”±æ­¤å¯¦é©—è­‰æ˜ï¼Œè—‰ç”±imitation learningä¾†åšæ¨¡å‹å£“ç¸®æ˜¯å¯è¡Œçš„ã€‚

## å¯¦é©—äºŒ :   **è­‰æ˜ASAF-1æ˜¯åœ¨æ¨¡ä»¿expert**

æˆ‘å€‘ä¹Ÿåšäº†è·Ÿè«–æ–‡ä¸­ä¸€æ¨£çš„å¯¦é©—ï¼Œè­‰æ˜ASAF-1æœƒå› æ¨¡ä»¿çš„expertæ°´æº–ä¸åŒè€Œæœ‰ä¸åŒçš„è¨“ç·´çµæœï¼Œä¸¦ä¸”æˆ‘å€‘é‚„å¤šå˜—è©¦äº†PyBullet InvertedDoublePendulumBulletEnv-v0é€™å€‹ç’°å¢ƒã€‚Ant-v2çš„expertåŒä¸Šç‚º[TD3](https://arxiv.org/pdf/1802.09477.pdf)ï¼ŒInvertedDoublePendulumBulletEnv-v0å‰‡ç‚º[SAC](https://arxiv.org/abs/1801.01290)ï¼Œæœ€å¾Œè¨“ç·´çµæœåœ–å¦‚ä¸‹å…©åœ–æ‰€ç¤º ( R = xxxx è¡¨ç¤ºexpertçš„rewardæ°´æº–ç´„åœ¨ xxxx $\pm$ $250$ ä¹‹é–“ )ã€‚

åœ¨å¯¦é©—éç¨‹ä¸­ï¼Œæˆ‘å€‘ç™¼ç¾**ç©©å®šä¸€è‡´çš„è¨“ç·´trajectoryç›¸ç•¶é‡è¦**ï¼Œä¸¦ä¸æ˜¯rewardå¹³å‡èµ·ä¾†æ¥è¿‘æˆ‘è¦çš„æ•¸å­—å³å¯ï¼Œä¹Ÿå¿…é ˆæ³¨æ„è®Šç•°æ•¸æ˜¯å¦éå¤§ï¼Œè‹¥trajectoryå°æ‡‰çš„rewardè®Šç•°éå¤§æœƒå°è‡´è¨“ç·´éç¨‹å¾ˆä¸ç©©å®šé€ æˆæ”¶æ–‚ç·©æ…¢ï¼Œå·¦å³å…©åœ–å°±å‘ˆç¾æ˜é¡¯çš„å°æ¯”ï¼Œå·¦åœ–å› ç‚ºrewardè®Šç•°è¼ƒå°ï¼Œå› æ­¤åœ–è¼ƒç‚ºå¹³ç©©ï¼Œè€Œå³åœ–å› rewardè®Šç•°è¼ƒå¤§ï¼Œè¨“ç·´éç¨‹å¾ˆä¸ç©©å®šã€‚ä¸éé€™ä¹Ÿæ˜¯å‡¸é¡¯å‡ºASAF-1èƒ½å¿ å¯¦æ¨¡ä»¿expertçš„æ€§è³ªå°±æ˜¯ã€‚

![SS.png](image/SS%201.png)

![SS.png](image/SS%202.png)

# é€éä¸€å€‹Toy Problemä¾†é©—è­‰è«–æ–‡ä¸­çš„Structure Discriminator

**è¨­è¨ˆä¸€å€‹ç‰¹æ®ŠGenerator ( G )**

ASAFæ´»ç”¨äº†ä¸€äº›GANçš„æ€§è³ªï¼Œæå‡ºstructured discriminatorä½¿å¾—ç„¡discriminator networkçš„GANè®Šæˆå¯èƒ½ ( æˆ–è€…èªªdiscriminatoræ˜¯implicit )ã€‚

ä½†æ˜¯**ä½¿ç”¨structured discriminatoræœ‰å€‹å…ˆæ±ºæ¢ä»¶ï¼Œå°±æ˜¯å¿…é ˆè¦å…ˆçŸ¥é“generatorç”¢ç”ŸæŸä¸€ç­†dataçš„æ©Ÿç‡åˆ†å¸ƒ !** é€™å°æ–¼å¹³å¸¸æˆ‘å€‘çœ‹åˆ°çš„GANä¾†èªªä¼¼ä¹é›£ä»¥åšåˆ°ï¼Œå°±ä»¥ç”Ÿæˆäººè‡‰çš„GANä¾†èªªï¼Œä»Šå¤©å¦‚æœä½ æœ‰ä¸€å¼µäººè‡‰åœ–ç‰‡ï¼Œè«‹å•ä½ å¯ä»¥æº–ç¢ºç®—å‡ºé€™å¼µäººè‡‰è¢«generatorç”Ÿæˆçš„æ©Ÿç‡å€¼æ˜¯å¤šå°‘å— ? ä¼¼ä¹ä¸å¤ªå¯èƒ½ã€‚

ASAFä¹‹æ‰€ä»¥å¯ä»¥ä½¿ç”¨structured discriminatorï¼Œæ˜¯å› ç‚ºæˆ‘å€‘çŸ¥é“trajectoryçš„æ©Ÿç‡è¡¨ç¤ºæ³•ï¼Œä¸¦ä¸”ç¶“éåˆ†å­åˆ†æ¯ç´„åˆ†ï¼Œstructured discriminatorç›´æ¥è®Šæˆäº†policyçš„å‡½å¼ï¼Œå› æ­¤structured discriminatorçš„è¼¸å‡ºå€¼å¯ä»¥è¼•æ˜“å¾—åˆ°ï¼Œæˆ‘å€‘æ‰èƒ½é€²è€Œå°policyåšè¨“ç·´ã€‚

ç‚ºäº†ç†è§£é€™å€‹å•é¡Œï¼Œæˆ‘å€‘ç°¡å–®åœ°è¨­è¨ˆäº†ä¸€å€‹reparameterized generatorï¼Œå®ƒçš„è¼¸å…¥æ˜¯$noise \sim N(0,1)$; è¼¸å‡ºæ˜¯$x$ $\sim$ $N(\mu, e^{\sigma})$ï¼Œè¼¸å‡ºå±¤é€™æ¨£è¨­è¨ˆçš„ç›®çš„æ˜¯ç‚ºäº†å¯ä»¥é€énormal distributionçš„å…¬å¼åæ±‚dataè¢«generatorç”¢ç”Ÿçš„æ©Ÿç‡å€¼ã€‚Discriminatorçš„éƒ¨åˆ†è‡ªç„¶æ˜¯ä½¿ç”¨æˆ‘å€‘è¦æ¢è¨çš„structured discriminatorã€‚è€Œç”¨ä¾†è¨“ç·´é€™å€‹GANçš„real dataå‰‡æ˜¯äºŒç¶­normal distribution $(x_1,x_2)$ï¼Œå…¶ä¸­$x_1 \sim N(-10, 0.5)$  ,  $x_2 \sim N(0.1, 7)$ã€‚

![Untitled Diagram.svg](image/Untitled_Diagram.svg)

**è¨“ç·´éç¨‹**

åœ–ç‰‡æ™‚é–“é †åºæ˜¯ç”±ä¸Šè€Œä¸‹ï¼Œå…¶ä¸­**è—é»**ç‚º**ç”Ÿæˆdata**ï¼Œ**ç´…é»**ç‚º**çœŸå¯¦data**ã€‚å¯ä»¥çœ‹åˆ°ä¸€é–‹å§‹ç”Ÿæˆdataèˆ‡çœŸå¯¦dataçš„åˆ†å¸ƒç›¸å·®ç”šé ï¼Œä½†ç¶“éè¨“ç·´å¾Œç”Ÿæˆdataå¹¾ä¹èˆ‡çœŸå¯¦dataçš„åˆ†å¸ƒç›¸åŒï¼Œå¯¦é©—è­‰æ˜åªè¦æœ‰è¾¦æ³•çŸ¥é“dataçš„ç”Ÿæˆæ©Ÿç‡å°±å¯ä»¥ä½¿ç”¨structured discriminatorçš„æŠ€å·§ï¼Œè¨“ç·´GANæ™‚å°±å¯ä»¥ä¸éœ€è¦discriminator networkã€‚ä¸éåœ¨å¯¦éš›æƒ…æ³ä¸­ï¼Œdataçš„distributioné¡¯ç„¶æœƒé æ¯”å¯¦é©—ä¸­çš„äºŒç¶­normal distributionè¤‡é›œå¾—å¤šï¼Œå› æ­¤**é€™å€‹toy problemåƒ…æ˜¯èªªæ˜dataç”Ÿæˆæ©Ÿç‡å°structured discriminatorçš„å¿…è¦æ€§**ï¼Œè€Œéæå‡ºä¸€å€‹generalçš„GANè§£æ±ºæ–¹æ¡ˆã€‚è‡³æ–¼é€™é …æŠ€å·§æ˜¯å¦æœ‰æ›´å¤šimitation learningä»¥å¤–çš„æ‡‰ç”¨ï¼Œæœ‰å¾…å¾ŒçºŒç ”ç©¶ (ä¹Ÿæœ‰å¯èƒ½å·²ç¶“æœ‰åªæ˜¯æˆ‘æ²’çœ‹åˆ°å°±æ˜¯)ã€‚

![AA.png](image/AA%202.png)

![AA.png](image/AA%203.png)

![AA.png](image/AA%204.png)

![AA.png](image/AA%205.png)

![åœ–ç‰‡ 93.png](image/%E5%9C%96%E7%89%87_93.png)

![åœ–ç‰‡ 93.png](image/%E5%9C%96%E7%89%87_93%201.png)
```
